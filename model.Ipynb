{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e283196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    adjusted_rand_score, \n",
    "    silhouette_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1018949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución original de clases:\n",
      "class\n",
      "unacc    1210\n",
      "acc       384\n",
      "good       69\n",
      "vgood      65\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar dataset\n",
    "car_evaluation = fetch_ucirepo(id=19)\n",
    "X = car_evaluation.data.features\n",
    "y = car_evaluation.data.targets\n",
    "\n",
    "# Verificar distribución de clases\n",
    "print(\"\\nDistribución original de clases:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ca0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocesamiento\n",
    "# Codificación de características ordinales\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        ['low', 'med', 'high', 'vhigh'],  # buying\n",
    "        ['low', 'med', 'high', 'vhigh'],  # maint\n",
    "        ['2', '3', '4', '5more'],         # doors\n",
    "        ['2', '4', 'more'],               # persons\n",
    "        ['small', 'med', 'big'],          # lug_boot\n",
    "        ['low', 'med', 'high']            # safety\n",
    "    ]\n",
    ")\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Codificación de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dfbbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. División estratificada de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, \n",
    "    y_encoded, \n",
    "    test_size=0.3, \n",
    "    stratify=y_encoded,  # Mantener distribución de clases\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e268c8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución después de SMOTE:\n",
      "2    847\n",
      "0    847\n",
      "1    847\n",
      "3    847\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 4. Balanceo de clases con SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nDistribución después de SMOTE:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ba2d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenando Random Forest ===\n",
      "\n",
      "Precisión: 0.9807\n",
      "F1-score (Macro): 0.9742\n",
      "F1-score (Weighted): 0.9808\n",
      "              precision  recall  f1-score   support\n",
      "acc              0.9487  0.9652    0.9569  115.0000\n",
      "good             0.9545  1.0000    0.9767   21.0000\n",
      "unacc            0.9917  0.9862    0.9890  363.0000\n",
      "vgood            1.0000  0.9500    0.9744   20.0000\n",
      "accuracy         0.9807  0.9807    0.9807    0.9807\n",
      "macro avg        0.9737  0.9754    0.9742  519.0000\n",
      "weighted avg     0.9810  0.9807    0.9808  519.0000\n",
      "\n",
      "=== Entrenando Árbol de Decisión ===\n",
      "\n",
      "Precisión: 0.9518\n",
      "F1-score (Macro): 0.8913\n",
      "F1-score (Weighted): 0.9535\n",
      "              precision  recall  f1-score   support\n",
      "acc              0.9027  0.8870    0.8947  115.0000\n",
      "good             0.7143  0.9524    0.8163   21.0000\n",
      "unacc            1.0000  0.9697    0.9846  363.0000\n",
      "vgood            0.7692  1.0000    0.8696   20.0000\n",
      "accuracy         0.9518  0.9518    0.9518    0.9518\n",
      "macro avg        0.8465  0.9523    0.8913  519.0000\n",
      "weighted avg     0.9580  0.9518    0.9535  519.0000\n",
      "\n",
      "=== Entrenando Red Neuronal ===\n",
      "\n",
      "Precisión: 0.9538\n",
      "F1-score (Macro): 0.9235\n",
      "F1-score (Weighted): 0.9542\n",
      "              precision  recall  f1-score   support\n",
      "acc              0.8966  0.9043    0.9004  115.0000\n",
      "good             0.8000  0.9524    0.8696   21.0000\n",
      "unacc            0.9806  0.9725    0.9765  363.0000\n",
      "vgood            1.0000  0.9000    0.9474   20.0000\n",
      "accuracy         0.9538  0.9538    0.9538    0.9538\n",
      "macro avg        0.9193  0.9323    0.9235  519.0000\n",
      "weighted avg     0.9554  0.9538    0.9542  519.0000\n"
     ]
    }
   ],
   "source": [
    "# 5. Entrenamiento de modelos y comparación\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Árbol de Decisión\": DecisionTreeClassifier(\n",
    "        max_depth=8,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Red Neuronal\": MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        activation='relu',\n",
    "        early_stopping=True,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Diccionario para almacenar métricas de cada modelo\n",
    "model_metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Entrenando {name} ===\")\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicción\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluación\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(\n",
    "        y_test, \n",
    "        y_pred, \n",
    "        target_names=label_encoder.classes_, \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas para comparación\n",
    "    model_metrics[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_macro': report['macro avg']['f1-score'],\n",
    "        'f1_weighted': report['weighted avg']['f1-score']\n",
    "    }\n",
    "    \n",
    "    # Imprimir resultados individuales\n",
    "    print(f\"\\nPrecisión: {accuracy:.4f}\")\n",
    "    print(f\"F1-score (Macro): {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"F1-score (Weighted): {report['weighted avg']['f1-score']:.4f}\")\n",
    "    print(pd.DataFrame(report).transpose().round(4))\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=label_encoder.classes_,\n",
    "        yticklabels=label_encoder.classes_\n",
    "    )\n",
    "    plt.title(f'Matriz de Confusión - {name}')\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig(f'static/confusion_{name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Guardar modelo\n",
    "    joblib.dump(model, f'model/{name.lower().replace(\" \", \"_\")}_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca650d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARACIÓN FINAL DE MODELOS\n",
      "============================================================\n",
      "\n",
      "Resumen de métricas por modelo:\n",
      "                   accuracy  f1_macro  f1_weighted\n",
      "Random Forest      0.980732  0.974237     0.980792\n",
      "Red Neuronal       0.953757  0.923463     0.954186\n",
      "Árbol de Decisión  0.951830  0.891311     0.953457\n",
      "\n",
      "============================================================\n",
      "MEJOR MODELO: Random Forest\n",
      "Precisión: 0.9807\n",
      "F1-score (Macro): 0.9742\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. Comparación de modelos y selección del mejor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARACIÓN FINAL DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear DataFrame con las métricas\n",
    "results_df = pd.DataFrame.from_dict(model_metrics, orient='index')\n",
    "results_df = results_df.sort_values(by='accuracy', ascending=False)\n",
    "print(\"\\nResumen de métricas por modelo:\")\n",
    "print(results_df)\n",
    "\n",
    "# Identificar el mejor modelo en base a accuracy\n",
    "best_model_name = results_df.index[0]\n",
    "best_accuracy = results_df.iloc[0]['accuracy']\n",
    "best_f1_macro = results_df.iloc[0]['f1_macro']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"Precisión: {best_accuracy:.4f}\")\n",
    "print(f\"F1-score (Macro): {best_f1_macro:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Gráfico comparativo de precisión\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df['accuracy'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Comparación de Precisión entre Modelos')\n",
    "plt.ylabel('Precisión')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.xticks(rotation=15)\n",
    "for i, v in enumerate(results_df['accuracy']):\n",
    "    plt.text(i, v + 0.005, f'{v:.4f}', ha='center')\n",
    "plt.savefig('static/model_comparison.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a907bc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validación con Clustering ===\n",
      "Adjusted Rand Index: 0.01\n",
      "Silhouette Score: 0.16\n"
     ]
    }
   ],
   "source": [
    "# 7. Clustering de validación\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_encoded)\n",
    "\n",
    "# Métricas de clustering\n",
    "ari = adjusted_rand_score(y_encoded, clusters)\n",
    "silhouette = silhouette_score(X_encoded, clusters)\n",
    "print(f\"\\n=== Validación con Clustering ===\")\n",
    "print(f\"Adjusted Rand Index: {ari:.2f}\")\n",
    "print(f\"Silhouette Score: {silhouette:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ef527fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/label_encoder.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Guardar encoders\n",
    "joblib.dump(encoder, 'model/encoder.pkl')\n",
    "joblib.dump(label_encoder, 'model/label_encoder.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
